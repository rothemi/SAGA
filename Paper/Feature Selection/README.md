# Feature Selection
## SAGA classifier development 
The development of the predictive model of SAGA was implemented using the R package “caret” based on a support vector machine with a radial basis function kernel (method = "svmRadial"). Unless otherwise specified, all calls to functions mentioned in this paragraph belong to the “caret” package with key parameters specified in parentheses after the name of the function or directly discussed in the text. All computations were run on a c5.18xlarge Amazon Web Service EC2 instance with 72 cores and 144 Gb RAM running RStudio 1.1.456 and R 3.5.1. The data splitting and resampling scheme to assess the performance of the models and control for overfitting is outlined in Supplementary Figure 3. First, the quantile normalized and batch corrected expression matrix (36,226 annotated probes, 152 samples with known IVIM properties) was partitioned into a training set comprised of 70% of the samples (107 samples) and an independent test set of 30% of the samples (45 samples). The test set was not used at any point for feature selection or model tuning. To allocate samples to the test or training set the caret function “createDataPartition” (p=0.7) was used, which performs stratified sampling based on the class labels to keep the distribution of transforming and nontransforming samples equal between the training and test sets. Since a single training / test set split can lead to a biased assessment of model building and feature selection ten stratified random training / test set splits of the dataset were created and the complete model building pipeline was run for ten times for a more unbiased and reliable assessment of the predictive modeling process. Predictive performance of many models, especially support vector machines, can be significantly affected by large numbers of irrelevant predictors. Furthermore, models using fewer predictors are quicker to compute, less prone to overfitting and generally better interpretable than models based on thousands of predictors. Therefore, a combination of feature selection steps was performed to reduce the number of predictors as far as possible while maintaining or increasing predictive power. First, we applied an unsupervised filter to each training set to exclude probes interrogating genes that were not expressed at all or show only little variation in the dataset. This step helped to reduce computation time and avoided the selection of features by the subsequent SVM-RFE step that have a good discriminatory power between the classes based on their AUROC, but display only a small absolute fold-change between the different classes. The R-package “genefilter” was used to discard probes with an interquartile range (IQR) of log2-expression values less than 0.8 in the quantile-normalized and batch corrected training cohort, which retained a median of 1,195 out of 36,226 annotated probes (Supplementary Data 4 tab 1). IQR = 0.8 was chosen empirically, since it consistently selected around 1,000 features in all test/training set splits. Setting the IQR lower (e.g. IQR= 0.5) retained too many features (median around 4,500), leading to a substantial increase in overall computation time as well as a failure to reduce the number of features in the subsequent SVM-RFE step in 3 out of 10 training/test splits. In contrast, setting IQR=1.2 selected on average around 250 features, which could be efficiently handled by SVM-RFE. However, at IQR=1.2 important predictors, such as A_55_P2077048/Itih5 (AUROC= 0.98) were already discarded before the actual feature selection step. The implementations using IQR 0.5 and IQR = 1.2 are available at GITHUB.  Next, we performed recursive feature elimination (SVM-RFE) on the training set using the function “rfe”. Since feature selection is part of the model building process, it needs to be conducted inside of a resampling layer (“external resampling layer”, Supplementary Figure 3) to assess the impact of the selection process on the model performance and to prevent overfitting of the model to the predictors. To establish the external resampling layer, 200 resamples of the training set were created by twenty times repeated 10-fold cross-validation using the function “createMultiFolds” (Parameters: k=10, times = 20). The function divides the entire training set (107 samples) into 10 subsets (folds) of equal size and the first fold (11 samples, “external holdouts”) is predicted by a model fit to the remaining 9 folds (96 samples, “external training”) of the data. This is repeated with the second fold after the first one has been returned to the training set and so on, resulting in 10 resamples for each of the twenty repeats of 10-fold CV. Importantly, the 200 identical resamples were used to fit the full models using all predictors, to allow a direct comparison of the SVM-RFE model and the full model using the resampling accuracies. The 200 resamples were submitted to the helper function “rfeControl”, which controls the details of the external resampling process of the function “rfe”. The feature selection process itself was carried out for each of the 200 resamples separately and computed in parallel by setting the “rfeControl” parameter: “allowParallel = TRUE”. To ensure reproducibility of the analysis, a fixed set of random seeds that “rfe” uses at each resampling iteration was created and submitted to “rfeControl” via the “seeds” parameter. Within each resample, SVM-RFE ranks all predictors according to their individual receiver operating characteristic (ROC) on the 96 training samples. In each iteration less important predictors are removed, the model is fitted to the 96 training samples and the 11 holdout samples are predicted.  The metric to be maximized by “rfe” was set to “Accuracy”.  After initial inspection of the resampling profiles, we noted that accuracy peaked most often between 5-30 predictors. For maximum resolution within these ranges, all subset sizes from 1-40 predictors were tested. Outside of this range, wider intervals were used (45, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500 predictors), resulting in 52 subset sizes in total. For each tested subset within each resample of the external layer an additional “inner layer” of resampling had to be established to determine the tuning parameters of the SVM-model. The details of the inner resampling layer were specified by the helper function “trainControl” and set to three times repeated 10-fold cross-validation (30 resamples). To be precise, each training set from the external layer (96 samples) was partitioned further into 30 internal resamples comprised of 86 “internal training” and 10 “internal holdout” samples, respectively (Supplementary Figure 3). For each value of tuning parameters and each internal resample, the SVMrad model was fit to the 86 internal training samples and the remaining 10 internal holdout samples were predicted. The prediction accuracy from the 30 internal resamples over the different tested hyperparameter values was used to determine the optimal value for the tuning parameters and these parameters were passed to the external layer to fit the model and predict the external hold-outs. SVM-RFE with a radial basis function kernel has two tuning parameters: cost (penalty parameter) and sigma (inverse width of the gaussian kernel). For the cost parameter, the parameter “tuneLength” of the “rfe”- function was set to 20, resulting in cost values ranging from 2-2 - 217. For the sigma parameter an analytical estimate was used which is calculated by “rfe” internally by calling the function “sigest” from the R-package kernlab. “sigest” uses the methodology proposed by Caputo et al to estimate a value for sigma which results in a good prediction performance when used with a radial kernel SVM. We validated this approach initially by a manual search for sigma over a wide range of values (2-15 - 20), but could not find substantially better solutions for our dataset than suggested by “sigest” (data not shown). Hence, using a fixed value for sigma estimated with “sigest” and tuning the SVM over the cost parameter only resulted in a substantially smaller hyperparameter space and reduced computation time for SVM-RFE. To find the best subset size for the entire training set, the prediction accuracy of the external holdout samples for each subset size and each resample was averaged into a resampling profile (Fig. 3b, Fig. 4a), which allowed to determine the best average subset size across all resamples. To generate the final set of predictors, “rfe” repeated the process on the complete training set with the optimal subset size determined from the resampling profile. The performance of the SVM-RFE model was compared to the full model using all predictors using the caret functions “resamples” and “diff”, which compare resampling results of different models on a common data set comprised of identical resamples using a paired t-test. The resampling-based results for the ten training / test set splits and the final model are tabulated in Supplementary Data 4 tab 1 (P-value_Resampling_full_vs_rfe). The GA procedure was implemented using the function “gafs” and its helper function “gafsControl” from the R package “caret”. The gene expression matrix reduced to the probes found by the preceding SVM-RFE step was used as input into GA. Similarly to the SVM-RFE implementation, SVM-GA was conducted inside an external resampling layer to assess the performance of the GA-model over the generations (external resampling accuracy). 50 external resamples of the training sets or the final dataset were created with the function “createMultiFolds” (k=10, times = 5) and passed to the function “gafsControl”, which controls the outer resampling process of the GA. The computational burden of SVM-GA is higher than for SVM-RFE, so only 50 external resamples were used to complete the analysis in a reasonable amount of time. The prediction performances on the external hold-out samples at each generation across all external resamples were averaged into the external resampling profile (Fig. 3 c, Fig. 4b), which was used to determine the optimal number of iterations the algorithm should proceed (Supplementary Figure 3). To determine the final feature set, “gafs” applied the GA to the entire training set for the optimal number of generations from the resampling process. Further parameters of “gafsControl” were set to enable parallel computing for the external layer, to maximize the test statistic (accuracy) and to use fixed random seeds for reproducibility. In initial runs using the default settings of “gaf” feature reduction was quite inefficient, leading to the removal of only 3-5 predictors on average. For a more effective reduction of feature numbers the size of the initial predictor subsets (chromosomes) in the starting population was reduced. Therefore, the helper function of GA (caretGA$initial) that creates the initial population was modified to produce chromosomes comprised of a random 40% of predictors, instead of creating initial subsets ranging from 10% to 90% of predictors. The GA procedure itself was run for 40 generations, with a population size of 40, a crossover probability of 0.7, a mutation probability of 0.1. Elitism was set to 3, meaning that the best three solutions survive to the next generation. The metric to optimize was set to “accuracy”, the classification method to “svmRadial”. Similarly to the SVM-RFE process, the GA had an additional inner layer of resampling conducted at each generation within each resample and for each chromosome to tune the SVM. The inner resampling layer of GA was set to two times repeated 10-fold cross-validation (20 resamples) by the helper function “trainControl”. For the cost parameter of the SVM the parameter “tuneLength” was set to 12, for cost values between 2-2 – 29. The reduced tune length was chosen to save computation time after it had been determined from the preceding steps that the optimal cost parameter for the SVM was in the range of 2-2 - 27.  For the sigma parameter the estimate computed by “sigest” function from “kernlab” was used as described above. For the analysis of gene expression of the selected predictors across murine haematopoiesis (20 probes from SVM-RFE and 1243 probes after unsupervised filtering, Supplementary Data 4 tab 5, Fig. 4e, Supplementary Figure 7) the online resource of the Immunological Genome Consortium32 (http://rstats.immgen.org/MyGeneSet_New/index.html) was queried using the corresponding gene symbols of the probes as input. 
## Estimation of classifier performance 
Samples in the test sets were predicted after training a support vector machine with radial kernel on the training set using all predictors (full model) or reduced to the optimal predictors found by SVM-RFE and SVM-GA (reduced models) by using the caret functions “train” and “predict”, respectively. For training the full and the reduced SVM-models identical parameters and resamples were specified in the “train” function (method = "svmRadial", metric = "Accuracy", tuneLength = 20, twenty repeats of 10-fold cross-validation). The function “predict” was used with the parameter “type” set to “prob”, which computes the probability that a sample belongs to a given class. An unknown sample was considered belonging to the class “transforming” when the probability for class “transforming” was greater than 0.5. Performance estimates (sensitivity, specificity, accuracy, kappa) for the predicted test sets were computed using the function “confusionMatrix” on the predicted and the true class labels, respectively (Supplementary Data 4 tab 2). For Figure 3d-f the resampling accuracies and their confidence intervals were determined using the function “resamples” for the full models, SVM-RFE and SVM-GA and plotted on the y-axis. The values on the x-axis represent the test set accuracies and the corresponding confidence intervals as output by the function “confusionMatrix”. The “pROC” R-package (v1.15.3) was used to compute and visualize the ROC curves for the test sets using the function “roc” on the probability for class “transforming” as output by the “predict” function. P values to compare the difference between the AUROC of two unpaired ROC curves were performed with the “roc.test” function using the “delong” method and the alternative hypothesis set to “greater”. Precision recall curves were generated using the R-package “PRROC” (v.1.3.1). As delineated in the main text, we defined SAGA as the compound model based on the predictions from SVM-RFE when this process yielded equal or less than 10 optimal predictors and from SVM-RFE followed by SVM-GA otherwise. For Figure 3g-j the prediction results for SAGA for all independent test sets using the 10 random test set approach (Figure 3g,h) or the batch-wise test set approach (19 batches, Figure 3i,j) were aggregated and compared to the performance of the IVIM assay via AUROC, AUPRC and calculation of the confusion matrices and associated performance estimates (Supplementary Data 4 tab 3 and Supplementary Data 5 tab 3). 
## Batch-wise construction/validation of SAGA
Raw intensities of 169 arrays from 19 experimental batches were read in and combined into an “EListRaw” object without further modification. 15 samples with unknown ground truth were subsequently removed from the dataset, resulting in 154 assays including two mock duplicates (X6374.1, X6379.1 from batch 17, Supplementary Data 7).  For iteration 1, batch 1 (IVIM #120411) was set aside as independent test set, all other batches (2-19) were used as training set and were quantile normalized, averaged and batch corrected as described above. The preprocessed training data was subjected to SVM-RFE and SVM-GA using the same parameters as above, except for the numbers of subset sizes to assess during SVM-RFE, which were reduced to 1,2,3…,40,45,50, all predictors = 43 predictor subsets in total to limit computational costs. After having determined the optimal predictors in the training set, quantile normalization, averaging and batch correction of the raw-training set was performed again using the bapred package, followed by separate add-on quantile normalization and add-on batch correction of the raw-test set. Add-on preprocessing handles training and test sets separately and prevents the alteration of the training set by the addition of test set samples (information leakage) and vice versa. This allows to keep the dataset, on which the classifier has been trained and optimized, fixed and is also used in the SAGA package as outlined below. The add-on preprocessed training and test sets were reduced to the number of optimal predictors determined on the training set (e.g. for the first iteration: 8 predictors). The SVM was trained on the reduced training set and the test samples were predicted as described in detail in the paragraph “Estimation of classifier performance” above. The complete procedure was repeated 18 additional times with every available batch to be used one time as independent test set. All results from the 19 iterations of building SAGA and predicting the hold-out batch are in Supplementary Data 5.


## Availability of raw data and R workspace file

All files can be downloaded [here](https://www.dropbox.com/sh/6c1ihn8adaii631/AABdkwv7IbcGOu3SWRSRxzV8a?dl=0)
